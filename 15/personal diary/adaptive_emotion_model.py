import os
import json
import joblib
from datetime import datetime
from typing import Dict, Optional

import torch
import numpy as np
import pandas as pd
from datasets import Dataset
from torch.nn import CrossEntropyLoss
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
)
from peft import get_peft_model, LoraConfig, TaskType

# –æ—Ç–∫–ª—é—á–∞–µ–º wandb
os.environ["WANDB_DISABLED"] = "true"


class AdaptiveEmotionModel:
    def __init__(self, model_dir: str, device: str = None):
        if device is None:
            device = "cuda" if torch.cuda.is_available() else "cpu"

        self.device = device
        self.model_dir = model_dir

        # –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–æ–æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤
        self.ft_dir = f"{self.model_dir}_fine_tuned"
        load_dir = self.ft_dir if os.path.exists(self.ft_dir) else self.model_dir

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
        self.tokenizer = AutoTokenizer.from_pretrained(load_dir)
        self.base_model = AutoModelForSequenceClassification.from_pretrained(
            load_dir
        ).to(device)

        # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è (–∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –ø–∞–ø–∫–∏ –º–æ–¥–µ–ª–∏)
        with open(
            os.path.join(model_dir, "config_custom.json"), "r", encoding="utf-8"
        ) as f:
            self.config = json.load(f)

        # label_encoder (–µ—Å–ª–∏ –µ—Å—Ç—å)
        label_encoder_path = os.path.join(model_dir, "label_encoder.joblib")
        if os.path.exists(label_encoder_path):
            try:
                self.label_encoder = joblib.load(label_encoder_path)
            except Exception as e:
                print(f" –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å label_encoder.joblib: {e}")
                self.label_encoder = None
        else:
            print(" label_encoder.joblib –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–¥–æ–ª–∂–∞—é –±–µ–∑ –Ω–µ–≥–æ")
            self.label_encoder = None

        self.id2label = {int(k): v for k, v in self.config["id2label"].items()}
        self.label2id = self.config["label2id"]
        self.num_labels = self.config["num_labels"]
        self.class_weights = torch.tensor(
            self.config["class_weights"], dtype=torch.float, device=device
        )

        # –ò—Å—Ç–æ—Ä–∏—è feedback'–æ–≤
        self.feedback_file = os.path.join(model_dir, "feedback_history.json")
        self.feedback_history = []
        self._load_feedback_history()

        # –ê–≤—Ç–æ‚Äë–¥–æ–æ–±—É—á–µ–Ω–∏–µ
        self.auto_fine_tune_threshold = 10  # –∫–∞–∂–¥—ã–µ 10 –Ω–æ–≤—ã—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π
        self.last_fine_tune_count = len(self.feedback_history)

        print(f"‚úì –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {load_dir} –Ω–∞ {device}")
        print(f"‚úì –ö–ª–∞—Å—Å—ã —ç–º–æ—Ü–∏–π: {self.config['labels']}")
        print(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ feedback-–æ–≤: {len(self.feedback_history)}\n")

    # ===== PREDICT =====

    def predict(self, text: str, return_probs: bool = True) -> Dict:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —ç–º–æ—Ü–∏–∏ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞."""
        self.base_model.eval()

        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            padding=True,
            max_length=128,
        ).to(self.device)

        with torch.no_grad():
            outputs = self.base_model(**inputs)
            probs = torch.softmax(outputs.logits, dim=-1)[0].cpu().numpy()

        pred_id = int(probs.argmax())
        pred_label = self.id2label[pred_id]
        confidence = float(probs[pred_id])

        result = {
            "emotion": pred_label,      # —Å—é–¥–∞ —Å–º–æ—Ç—Ä–∏—Ç —Ñ—Ä–æ–Ω—Ç
            "confidence": confidence,   # 0..1
            "id": pred_id,
        }

        if return_probs:
            result["probs"] = {
                self.id2label[i]: float(probs[i]) for i in range(len(probs))
            }

        return result

    # ===== FEEDBACK =====

    def add_feedback(
        self, text: str, predicted_emotion: str, corrected_emotion: str
    ) -> bool:
        """
        –î–æ–±–∞–≤–ª—è–µ—Ç feedback –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –∞–≤—Ç–æ‚Äë–¥–æ–æ–±—É—á–µ–Ω–∏–µ.

        text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç
        predicted_emotion: —è—Ä–ª—ã–∫ –º–æ–¥–µ–ª–∏ (—Ä—É—Å—Å–∫–∏–π, –∫–∞–∫ –≤ id2label / label2id)
        corrected_emotion: –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —è—Ä–ª—ã–∫ (–¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –∫–ª—é—á–æ–º label2id)
        """
        if corrected_emotion not in self.label2id:
            print(f"–≠–º–æ—Ü–∏—è '{corrected_emotion}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —Å–ø–∏—Å–∫–µ –∫–ª–∞—Å—Å–æ–≤")
            print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ: {list(self.label2id.keys())}")
            return False

        entry = {
            "text": text,
            "predicted_emotion": predicted_emotion,
            "corrected_emotion": corrected_emotion,
            "timestamp": datetime.now().isoformat(),
        }
        self.feedback_history.append(entry)
        self._save_feedback_history()

        print(f"‚úì Feedback –¥–æ–±–∞–≤–ª–µ–Ω: {predicted_emotion} ‚Üí {corrected_emotion}")

        # === –ê–í–¢–û‚Äë–î–û–û–ë–£–ß–ï–ù–ò–ï ===
        new_count = len(self.feedback_history)
        if new_count >= self.auto_fine_tune_threshold:
            print(
                f"üöÄ –î–æ—Å—Ç–∏–≥–Ω—É—Ç –ø–æ—Ä–æ–≥ auto fine-tune: {new_count} feedback'–æ–≤. "
                f"–ó–∞–ø—É—Å–∫–∞—é –¥–æ–æ–±—É—á–µ–Ω–∏–µ (—É—Å–∏–ª–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º)..."
            )
            success = self.fine_tune()
            if success:
                # –ü–æ–¥–≥—Ä—É–∂–∞–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
                if os.path.exists(self.ft_dir):
                    self.base_model = AutoModelForSequenceClassification.from_pretrained(
                        self.ft_dir
                    ).to(self.device)
                    self.tokenizer = AutoTokenizer.from_pretrained(self.ft_dir)
                    print("‚úì self.base_model –æ–±–Ω–æ–≤–ª–µ–Ω–∞ –¥–æ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏")
                # –ø–æ—Å–ª–µ fine_tune feedback_history —É–∂–µ –æ—á–∏—â–µ–Ω
                self.last_fine_tune_count = len(self.feedback_history)
            else:
                print("auto fine-tune –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –∏–ª–∏ –æ—à–∏–±–∫–∞)")

        return True

    def _save_feedback_history(self):
        os.makedirs(os.path.dirname(self.feedback_file), exist_ok=True)
        with open(self.feedback_file, "w", encoding="utf-8") as f:
            json.dump(self.feedback_history, f, ensure_ascii=False, indent=2)

    def _load_feedback_history(self):
        if os.path.exists(self.feedback_file):
            with open(self.feedback_file, "r", encoding="utf-8") as f:
                self.feedback_history = json.load(f)
        else:
            self.feedback_history = []

    # ===== DATASET PREPARATION =====

    def _prepare_fine_tune_dataset(self) -> Optional[Dataset]:
        if not self.feedback_history:
            print("–ù–µ—Ç feedback'–æ–≤ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è")
            return None

        df = pd.DataFrame(self.feedback_history)
        df["label_id"] = df["corrected_emotion"].map(self.label2id)
        df = df.dropna(subset=["label_id"]).reset_index(drop=True)

        if len(df) == 0:
            print("–ü–æ—Å–ª–µ –º–∞–ø–ø–∏–Ω–≥–∞ –¥–∞—Ç–∞—Å–µ—Ç –ø—É—Å—Ç (–ø—Ä–æ–≤–µ—Ä—å –Ω–∞–∑–≤–∞–Ω–∏—è —ç–º–æ—Ü–∏–π)")
            return None

        print(f"–ü—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è: {len(df)}")
        print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã—Ö —ç–º–æ—Ü–∏–π:")
        print(df["corrected_emotion"].value_counts())

        dataset = Dataset.from_pandas(df[["text", "label_id"]])
        return dataset

    # ===== FINE-TUNING (—É—Å–∏–ª–µ–Ω–Ω—ã–π) =====

    def fine_tune(
        self, num_epochs: int = 3, learning_rate: float = 5e-5, use_lora: bool = True
    ) -> bool:
        """
        –î–æ–æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ –Ω–∞–∫–æ–ø–ª–µ–Ω–Ω—ã—Ö feedback'–∞—Ö.
        –£—Å–∏–ª–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º: –¥–µ–ª–∞–µ—Ç —Å–∏–ª—å–Ω—ã–π –∞–∫—Ü–µ–Ω—Ç –Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è—Ö.
        """
        feedback_dataset = self._prepare_fine_tune_dataset()
        if feedback_dataset is None or len(feedback_dataset) == 0:
            print("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è, fine_tune –ø—Ä–µ—Ä–≤–∞–Ω")
            return False

        dataset_size = len(feedback_dataset)

        # –î–ª—è –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ —É—Å–∏–ª–∏–≤–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ
        if dataset_size <= 50:
            num_epochs = max(num_epochs, 10)
            learning_rate = max(learning_rate, 2e-4)
            weight_decay = 0.0
        else:
            weight_decay = 0.01

        print(
            f"–ò—Å–ø–æ–ª—å–∑—É—é —É—Å–∏–ª–µ–Ω–Ω—ã–π fine-tune: "
            f"dataset_size={dataset_size}, epochs={num_epochs}, lr={learning_rate}, weight_decay={weight_decay}"
        )

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        def tokenize_function(examples):
            texts = [str(t) for t in examples["text"]]
            enc = self.tokenizer(
                texts,
                padding="max_length",
                truncation=True,
                max_length=128,
            )
            enc["labels"] = examples["label_id"]
            return enc

        dataset_encoded = feedback_dataset.map(
            tokenize_function,
            batched=True,
            batch_size=8,
            remove_columns=feedback_dataset.column_names,
        )
        dataset_encoded.set_format(type="torch")

        print(f"–¢–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(dataset_encoded)} –ø—Ä–∏–º–µ—Ä–æ–≤\n")

        model = self.base_model
        if use_lora:
            print("–ò—Å–ø–æ–ª—å–∑—É—é LoRA –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ fine-tuning (—É—Å–∏–ª–µ–Ω–Ω—ã–π —Ä–µ–∂–∏–º)")
            lora_config = LoraConfig(
                task_type=TaskType.SEQ_CLS,
                r=16,          # —Å–∏–ª—å–Ω–µ–µ –∞–¥–∞–ø—Ç–µ—Ä—ã
                lora_alpha=64,
                lora_dropout=0.1,
                bias="none",
                target_modules=["query", "value"],
            )
            model = get_peft_model(model, lora_config)
            model.print_trainable_parameters()

        outer_self = self

        class WeightedTrainer(Trainer):
            def compute_loss(
                inner_self, model, inputs, return_outputs: bool = False, **kwargs
            ):
                labels = inputs.pop("labels")
                outputs = model(**inputs)
                logits = outputs.logits

                # –ë–∞–∑–æ–≤—ã–µ –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤
                class_weights = outer_self.class_weights.clone().to(model.device)

                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ —É—Å–∏–ª–∏–≤–∞–µ–º –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤,
                # –∫–æ—Ç–æ—Ä—ã–µ —Ä–µ–∞–ª—å–Ω–æ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ —ç—Ç–æ–º –±–∞—Ç—á–µ
                with torch.no_grad():
                    unique_labels = torch.unique(labels)
                    for lbl in unique_labels:
                        class_weights[lbl] *= 2.0  # –≤ 2 —Ä–∞–∑–∞ —Å–∏–ª—å–Ω–µ–µ

                loss_fct = CrossEntropyLoss(weight=class_weights)
                loss = loss_fct(
                    logits.view(-1, outer_self.num_labels), labels.view(-1)
                )
                return (loss, outputs) if return_outputs else loss

        training_args = TrainingArguments(
            output_dir="./fine_tune_checkpoints",
            learning_rate=learning_rate,
            per_device_train_batch_size=8,
            num_train_epochs=num_epochs,
            weight_decay=weight_decay,
            logging_steps=10,
            save_strategy="no",
            seed=42,
            fp16=False,              # –Ω–∞ CPU
            remove_unused_columns=False,
            report_to="none",
        )

        trainer = WeightedTrainer(
            model=model,
            args=training_args,
            train_dataset=dataset_encoded,
            tokenizer=self.tokenizer,
            data_collator=DataCollatorWithPadding(self.tokenizer),
        )

        trainer.train()

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å
        os.makedirs(self.ft_dir, exist_ok=True)
        model.save_pretrained(self.ft_dir)
        self.tokenizer.save_pretrained(self.ft_dir)

        info = {
            "fine_tuned_date": datetime.now().isoformat(),
            "num_feedback_examples": len(feedback_dataset),
            "num_epochs": num_epochs,
            "learning_rate": learning_rate,
            "use_lora": use_lora,
            "mode": "strong_feedback",
        }
        with open(
            os.path.join(self.ft_dir, "fine_tune_info.json"), "w", encoding="utf-8"
        ) as f:
            json.dump(info, f, ensure_ascii=False, indent=2)

        print(f"\n –î–æ–æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {self.ft_dir}")

        # –û—á–∏—Å—Ç–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö feedback'–æ–≤
        used_count = len(feedback_dataset)
        if used_count > 0 and used_count <= len(self.feedback_history):
            self.feedback_history = self.feedback_history[used_count:]
            self._save_feedback_history()
            print(f"‚úì –£–¥–∞–ª–µ–Ω–æ {used_count} –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö feedback'–æ–≤")
        self.last_fine_tune_count = len(self.feedback_history)

        return True

    def get_feedback_stats(self) -> Dict:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ feedback'–∞–º."""
        if not self.feedback_history:
            return {"total": 0}

        df = pd.DataFrame(self.feedback_history)
        return {
            "total": len(df),
            "unique_texts": len(df["text"].unique()),
            "corrections_by_emotion": df["corrected_emotion"]
            .value_counts()
            .to_dict(),
            "misclassified_as": df["predicted_emotion"]
            .value_counts()
            .to_dict(),
        }
